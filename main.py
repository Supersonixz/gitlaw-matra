import os
import time
import json
import sys
import logging
import re
import glob

# --- Configuration ---
RAW_DIR = "raw"  # ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡πÄ‡∏Å‡πá‡∏ö PDF
JSON_DIR = "json"  # ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡πÄ‡∏Å‡πá‡∏ö JSON ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå


# Configure Logging
def setup_logging():
    logger = logging.getLogger()
    logger.setLevel(logging.INFO)
    if logger.hasHandlers():
        logger.handlers.clear()

    # Force UTF-8 for Windows Console
    if sys.platform == "win32":
        try:
            sys.stdout.reconfigure(encoding="utf-8")
            sys.stderr.reconfigure(encoding="utf-8")
        except AttributeError:
            pass

    handler = logging.StreamHandler(sys.stdout)
    handler.setFormatter(logging.Formatter("%(asctime)s - %(levelname)s - %(message)s"))
    logger.addHandler(handler)


setup_logging()

from config import CATEGORIES
from gemini_processor import process_pdf_with_gemini
from agents import AgentSummarizer

agent_summarizer = AgentSummarizer()


def get_year_from_filename(filename):
    """‡∏î‡∏∂‡∏á‡∏õ‡∏µ 4 ‡∏´‡∏•‡∏±‡∏Å‡∏à‡∏≤‡∏Å‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå ‡πÄ‡∏ä‡πà‡∏ô '2475-1.pdf' -> 2475"""
    match = re.search(r"(\d{4})", filename)
    if match:
        return int(match.group(1))
    return 0  # ‡∏´‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠


def process_pipeline(pdf_path):
    # 1. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Path ‡πÅ‡∏•‡∏∞‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå
    filename = os.path.basename(pdf_path)  # "2475-1.pdf"
    file_stem = os.path.splitext(filename)[0]  # "2475-1"
    year = get_year_from_filename(filename)  # 2475

    # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î path ‡πÑ‡∏ü‡∏•‡πå‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÉ‡∏ô folder json/
    output_json_path = os.path.join(JSON_DIR, f"{file_stem}.json")
    summary_json_path = os.path.join(JSON_DIR, f"summary_{file_stem}.json")

    logging.info(f"üöÄ Processing: {filename} (Year: {year})")

    # --- Step 1: PDF to JSON (Extraction) ---
    if os.path.exists(output_json_path):
        logging.info(f"‚ö° JSON exists at {output_json_path}. Skipping Extraction.")
        # ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡∏ï‡πà‡∏≠‡πÉ‡∏ô Step 2
        with open(output_json_path, "r", encoding="utf-8") as f:
            all_sections = json.load(f)
    else:
        # ‡∏™‡πà‡∏á Gemini ‡∏≠‡πà‡∏≤‡∏ô
        try:
            extracted_data = process_pdf_with_gemini(pdf_path)
            all_sections = (
                extracted_data
                if isinstance(extracted_data, list)
                else extracted_data.get("sections", [])
            )

            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå JSON ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö PDF (‡πÑ‡∏°‡πà‡∏ó‡∏±‡∏ö‡∏Å‡∏±‡∏ô‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô)
            with open(output_json_path, "w", encoding="utf-8") as f:
                json.dump(all_sections, f, ensure_ascii=False, indent=2)
            logging.info(f"üíæ Saved Raw Data to {output_json_path}")

        except Exception as e:
            logging.error(f"Failed to process PDF {filename}: {e}")
            return

    # --- Step 2: JSON to Summary ---
    if os.path.exists(summary_json_path):
        logging.info(f"‚ö° Summary exists at {summary_json_path}. Skipping Summary.")
        return

    logging.info(f"üìù Generating Summaries for {filename}...")
    generate_summaries_from_data(all_sections, year, summary_json_path)


def generate_summaries_from_data(sections, year, output_path):
    logging.info(f"‚ö° Single-Shot Summarization for Year {year}...")

    # 1. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Data Preparation)
    grouped_content_for_ai = {}
    grouped_sections_raw = {}
    for section in sections:
        cat_id = section.get("category_id", "uncategorized")

        # Init list
        if cat_id not in grouped_content_for_ai:
            grouped_content_for_ai[cat_id] = []
            grouped_sections_raw[cat_id] = []

        # ‡πÄ‡∏Å‡πá‡∏ö Text ‡πÉ‡∏´‡πâ AI (‡πÉ‡∏™‡πà‡πÄ‡∏•‡∏Ç‡∏°‡∏≤‡∏ï‡∏£‡∏≤‡πÉ‡∏´‡πâ‡∏≠‡πà‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢)
        text_with_ref = f"[‡∏°.{section.get('section_number')}] {section.get('content')}"
        grouped_content_for_ai[cat_id].append(text_with_ref)

        # ‡πÄ‡∏Å‡πá‡∏ö Object ‡∏î‡∏¥‡∏ö
        grouped_sections_raw[cat_id].append(
            {
                "section_number": section.get("section_number"),
                "content": section.get("content"),
            }
        )

    # 2. ‡∏™‡πà‡∏á‡∏á‡∏≤‡∏ô‡πÉ‡∏´‡πâ Agent (AI ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•)
    ai_results_dict = agent_summarizer.run_batch(grouped_content_for_ai)

    if not ai_results_dict:
        logging.error("‚ùå No summary data returned from AI. Skipping save.")
        return

    # 3. ‡∏£‡∏ß‡∏°‡∏£‡πà‡∏≤‡∏á (Merge AI Result + Raw Data)
    final_output_list = []

    for cat_id, cat_name in CATEGORIES.items():
        if cat_id not in grouped_content_for_ai:
            continue

        ai_data = ai_results_dict.get(cat_id, {})
        logging.info(
            f"   > Summarizing: {cat_id} ({len(grouped_content_for_ai[cat_id])} sections)"
        )

        final_output_list.append(
            {
                "constitution_year": year,
                "category_id": cat_id,
                "category_name": cat_name,
                # ‡∏™‡πà‡∏ß‡∏ô‡∏ö‡∏ó‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå
                "ai_summary": ai_data.get("summary", "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡∏∏‡∏õ"),
                "key_change": ai_data.get("key_change", "-"),
                "section_count": len(grouped_sections_raw[cat_id]),
                # ‡∏™‡πà‡∏ß‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô (Raw Data)
                "sections": grouped_sections_raw.get(cat_id, []),
            }
        )

    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(final_output_list, f, ensure_ascii=False, indent=2)

    logging.info(f"‚úÖ Saved Rich Summary (AI + Raw) to {output_path}")


if __name__ == "__main__":
    import sys

    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö API Key
    if not os.getenv("GOOGLE_API_KEY"):
        logging.error("CRITICAL: GOOGLE_API_KEY missing.")
        exit(1)

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á Folder ‡∏õ‡∏•‡∏≤‡∏¢‡∏ó‡∏≤‡∏á‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ
    os.makedirs(JSON_DIR, exist_ok=True)

    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Folder ‡∏ï‡πâ‡∏ô‡∏ó‡∏≤‡∏á
    if not os.path.exists(RAW_DIR):
        logging.error(
            f"Folder '{RAW_DIR}' not found. Please create it and put PDFs inside."
        )
        exit(1)

    # --- ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÉ‡∏´‡∏°‡πà: ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤ User ‡∏™‡∏±‡πà‡∏á‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏°‡∏≤‡πÑ‡∏´‡∏°? ---
    target_pdfs = []

    # ‡∏Å‡∏£‡∏ì‡∏µ 1: User ‡∏™‡∏±‡πà‡∏á‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á (‡πÄ‡∏ä‡πà‡∏ô python main.py 2475-1.pdf)
    if len(sys.argv) > 1:
        specific_filename = sys.argv[1]  # ‡∏£‡∏±‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏≤‡∏Å‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á
        specific_path = os.path.join(RAW_DIR, specific_filename)

        if os.path.exists(specific_path):
            target_pdfs = [specific_path]  # ‡∏ó‡∏≥‡πÅ‡∏Ñ‡πà‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
            print(f"üéØ Single Mode: Selected '{specific_filename}'")
        else:
            logging.error(f"‚ùå File not found in {RAW_DIR}: {specific_filename}")
            exit(1)

    # ‡∏Å‡∏£‡∏ì‡∏µ 2: User ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏™‡∏±‡πà‡∏á‡∏≠‡∏∞‡πÑ‡∏£‡πÄ‡∏•‡∏¢ -> ‡πÄ‡∏´‡∏°‡∏≤‡∏´‡∏°‡∏î (Batch Mode)
    else:
        target_pdfs = glob.glob(os.path.join(RAW_DIR, "*.pdf"))
        print(f"üìÇ Batch Mode: Found {len(target_pdfs)} PDFs. Processing all...")

    if not target_pdfs:
        logging.error(f"No PDF files to process.")
        exit(1)

    # --- ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡πÅ‡∏•‡∏∞‡∏´‡∏•‡∏≤‡∏¢‡πÑ‡∏ü‡∏•‡πå) ---
    for pdf_path in target_pdfs:
        print("-" * 50)
        process_pipeline(pdf_path)

    print("\nüéâ All Done! Check the 'json' folder.")
